{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA-Python with Numba\n",
    "\n",
    "### Computing the value of Pi with GPU-accelerated Monte Carlo\n",
    "\n",
    "Numba exposes the CUDA programming API which makes it easy to program GPUs directly from python.  Here we will compute the value of Pi via Monte Carlo. Recall that the area of a circle is $\\pi r^2$. If we circumscribe a square about the circle of radius $r$, the circumscribed square will have a sidelength of $2r$ and an area $4r^2$.  So the ratio $R$ of the area of the circle to the area of the square is $\\pi/4$.  Note that this is independent of the radius.  \n",
    "    \n",
    "To numerically compute this ratio $R$, we will construct our own circle inscribed within a square and, since we are doing this numerically, we choose a convenient radius of 1, and choose for the circle to be located at the origin. Sampling points in this 2D space means that we draw two uniformly distributed random numbers in the range \\[0, 1).  This corresponds to sampling in one quadrant of our figure, but the ratio is unchanged.  \n",
    "\n",
    "To determine if a point falls within the circle, we compute the distance - or vector norm - from the origin.  For a 2D point with coordinates $x$ and $y$, this is $(x^2 + y^2)^{1/2}$.  If the vector norm is less than 1, then the point falls within the circle.  If it's greater than 1, then it falls outside the circle.  The ratio $R$ is simply the fraction of points that fall within the circle over the total number of points that we have sampled.  Pi is then $4*R$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "import math\n",
    "import numpy as np\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CUDA Kernels\n",
    "\n",
    "`compute_norm` takes an array of 2D points and returns an array which indicates if the points fall within the circle of unit 1.  \n",
    "\n",
    "CUDA-provided variables `cuda.threadIdx.x`, `cuda.blockIdx.x`, and `cuda.blockDim.x` are used to index into the input array, `ar`.  Note the work performed by each thread.  In this kernel, each thread operates on a single point, computes its norm, and stores a single result.\n",
    "\n",
    "`sum_reduce` is a GPU-accelerated reduction operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def compute_norm(ar, res):\n",
    "    '''\n",
    "    determine if points fall within a circle of radius 1.0\n",
    "    ---\n",
    "    From a 2D array of shape (N,2), compute the vector norm\n",
    "    of each point.  If norm is less than 1.0, store True,\n",
    "    else, store False.\n",
    "    \n",
    "    Result array must be shaped (N)\n",
    "    '''\n",
    "    # Thread id in a 1D block\n",
    "    tx = cuda.threadIdx.x\n",
    "    # Block id in a 1D grid\n",
    "    bx = cuda.blockIdx.x\n",
    "    # Block width, i.e. number of threads per block\n",
    "    bw = cuda.blockDim.x\n",
    "    # Compute flattened index inside the array\n",
    "    px = tx + bx * bw\n",
    "    \n",
    "    if px < ar.size:  # Check array boundaries\n",
    "        \n",
    "        # compute vector norm (distance from origin)\n",
    "        norm = math.sqrt(ar[px, 0]**2 + ar[px, 1]**2)\n",
    "        res[px] = norm<1.0\n",
    "        \n",
    "@cuda.reduce\n",
    "def sum_reduce(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Input Data\n",
    "\n",
    "`N` holds the total number of points that we will sample.  The input data will be shaped (N, 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the size of our input array\n",
    "N = int(5e8)\n",
    "\n",
    "# generate our input array of random numbers\n",
    "P = np.random.random(size=(N, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy Implementation\n",
    "\n",
    "Let's first confirm that our algorithm works.  We'll implement it in numpy, and time it for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.141561248\n",
      "10.1 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TimeitResult : 10.1 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timeit -n1 -r1 -o\n",
    "In = np.linalg.norm(P, axis=1)<=1.0\n",
    "R = np.sum(In)/N\n",
    "print(R*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the timing result\n",
    "CPU_RESULT = _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numba Implementation\n",
    "\n",
    "First thing, we need to send the data to the GPU and allocate space for the result.  Note that we allocate space on the GPU for the result.  We'll transfer it back at the end.\n",
    "\n",
    "Next, we'll define enough threads to perform all the work necessary.  There are other strategies for doing this, but for simplicity, we'll just choose to make sure that `nblocks`*`nthreads`> N.\n",
    "\n",
    "We launch the kernel with our launch configuration to compute the boolean array stored in `result_gpu`.  Now we call the reduction kernel on the data already on the gpu, `result_gpu`.  The return value is the sum of all the True values and so $R$ is simply the number of True values over N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.141561248\n",
      "3.141561248\n",
      "3.141561248\n",
      "3.141561248\n",
      "3.141561248\n",
      "3.141561248\n",
      "3.141561248\n",
      "3.141561248\n",
      "3.141561248\n",
      "3.141561248\n",
      "1.08 s ± 104 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TimeitResult : 1.08 s ± 104 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timeit -n2 -r5 -o\n",
    "array_gpu = cuda.to_device(P)\n",
    "result_gpu = cuda.device_array(N, dtype=np.int32)\n",
    "\n",
    "nthreads=1024\n",
    "nblocks=math.floor(N/nthreads)+1\n",
    "\n",
    "# Launch the kernel\n",
    "compute_norm[nblocks, nthreads](array_gpu, result_gpu)\n",
    "\n",
    "R = sum_reduce(result_gpu)/N\n",
    "print(R*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_RESULT = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speedup factor:  9.331899758070819 X\n"
     ]
    }
   ],
   "source": [
    "print('Speedup factor: ', CPU_RESULT.average / GPU_RESULT.average, 'X')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
